---
title: "VQVAEã®æ¦‚è¦"
emoji: "ğŸ‘‹"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["æ©Ÿæ¢°å­¦ç¿’"]
published: false
---


# VQVAE

## Feature Quanrizationéƒ¨åˆ†

```python
# D(=1ã¤ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ)=64, K(=ä¿å­˜ã™ã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°)=10
emb_dim, num_emb = 64, 10

# Feature Quantization Moduleã«ã¯ã€ç•³ã¿è¾¼ã¿å±¤ã®å‡ºåŠ›ã§ã‚ã‚‹ç‰¹å¾´ãƒãƒƒãƒ—ã‚’ä½¿ç”¨ã™ã‚‹
# inputs: [B, C=D, H, W] --> [B, H, W, C=D]
inputs = inputs.permute(0, 2, 3, 1).contiguous()

# ç‰¹å¾´ãƒãƒƒãƒ—ã‚’1æ¬¡å…ƒã«å¤‰æ›ã™ã‚‹
# flatten inputs feature map: [B, H, W, D] --> [N(=BxHxW), D]
flatten = inputs.view(-1, emb_dim)

# è¾æ›¸æ•°åˆ†æ ¼ç´ã™ã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
# embedding weight: [D, K]
embedding = torch.randn(emb_dim, num_emb)

# å…¥åŠ›ã•ã‚ŒãŸç‰¹å¾´ãƒãƒƒãƒ—ã¨ã€è¾æ›¸ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¨ã®è·é›¢ã‚’è¨ˆç®—ã™ã‚‹
# flatten:   [N, D]
# embedding: [D, K]
# distance: [N, K] --> ç‰¹å¾´ãƒãƒƒãƒ—ã®1ã¤1ã¤ã®ç©ºé–“çš„ä½ç½®ã«å¯¾ã™ã‚‹è¾æ›¸ãƒ™ã‚¯ãƒˆãƒ«ã®è·é›¢
distance = (
    flatten.pow(2).sum(dim=1, keepdim=True)
    -2 * flatten @ embedding
    + embeddings.pow(2).sum(dim=0, keepdim=True)
)

# distanceã‹ã‚‰ã€1ã¤1ã¤ã®ç©ºé–“çš„ä½ç½®ã«å¯¾ã—ã¦ã€æœ€ã‚‚è¿‘ã„è¾æ›¸ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ã™ã‚‹
# embedding_idx: [N, K] --> [N, ]
embedding_idx = torch.argmin(distance, dim=1)

# è¾æ›¸ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒã‚’å…¥åŠ›ã•ã‚ŒãŸç‰¹å¾´ãƒãƒƒãƒ—ã«åˆã‚ã›ã‚‹
# embedding_idx: [N, ] --> [B, H, W, ]
embedding_idx = embedding_idx.view(*inputs.size()[:-1])

# 1ã¤1ã¤ã®ç©ºé–“çš„ä½ç½®ã«å¯¾å¿œã™ã‚‹è¾æ›¸Indexã«å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡ºã™ã‚‹
# è¾æ›¸Indexã¯[0, K-1]ã®å€¤ã‚’ã¨ã‚‹
# quantize: [B, H, W, ] x [K, D] --> [B, H, W, D]
quantize = F.embedding(embedding_idx, embedding.transpose(0, 1))

# é‡å­åŒ–ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’å…¥åŠ›ã•ã‚ŒãŸç‰¹å¾´ãƒãƒƒãƒ—ã«åˆã‚ã›ã‚‹
# quantize: [B, H, W, D] --> [B, H, W, D=C]
quantize = quantize.view(*inputs.size())
```

æå¤±å€¤ã¨å‹¾é…ä¼æ¬

```python
# é‡å­åŒ–å‰å¾Œã®ç‰¹å¾´ãƒãƒƒãƒ—ã®è¿‘ã¥ã‘ã‚‹
e_latent_loss = F.mse_loss(quantize.detach(), inputs         )
q_latent_loss = F.mse_loss(quantize,          inputs.detach())

# commitmentã®ä¿‚æ•°ã‚’è¨­ã‘ã‚‹
loss = q_latent_loss + commitment * e_latent_loss

# Decoderå´ã®é‡å­åŒ–ã•ã‚ŒãŸç‰¹å¾´ãƒãƒƒãƒ—ã«å¯¾ã™ã‚‹å‹¾é…ã‚’ãã®ã¾ã¾Encoderå´ã®å…¥åŠ›ã«æ¸¡ã™
# é€†ä¼æ¬ç”¨ã®è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã—ãªã„ã‚ˆã†ã«detach()ã‚’ä½¿ç”¨ã™ã‚‹
quantize = inputs + (quantize - inputs).detach()
```