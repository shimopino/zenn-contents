---
title: "VQVAEã®æ¦‚è¦"
emoji: "ğŸ‘‹"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["æ©Ÿæ¢°å­¦ç¿’"]
published: false
---


# VQVAE

## Feature Quanrizationéƒ¨åˆ†

```python
# D(=1ã¤ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ)=64, K(=ä¿å­˜ã™ã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°)=10
emb_dim, num_emb = 64, 10

# Feature Quantization Moduleã«ã¯ã€ç•³ã¿è¾¼ã¿å±¤ã®å‡ºåŠ›ã§ã‚ã‚‹ç‰¹å¾´ãƒãƒƒãƒ—ã‚’ä½¿ç”¨ã™ã‚‹
# inputs: [B, C=D, H, W] --> [B, H, W, C=D]
inputs = inputs.permute(0, 2, 3, 1).contiguous()

# ç‰¹å¾´ãƒãƒƒãƒ—ã‚’1æ¬¡å…ƒã«å¤‰æ›ã™ã‚‹
# flatten inputs feature map: [B, H, W, D] --> [N(=BxHxW), D]
flatten = inputs.view(-1, emb_dim)

# è¾æ›¸æ•°åˆ†æ ¼ç´ã™ã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
# embedding weight: [D, K]
embedding = torch.randn(emb_dim, num_emb)

# å…¥åŠ›ã•ã‚ŒãŸç‰¹å¾´ãƒãƒƒãƒ—ã¨ã€è¾æ›¸ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¨ã®è·é›¢ã‚’è¨ˆç®—ã™ã‚‹
# flatten:   [N, D]
# embedding: [D, K]
# distance: [N, K] --> ç‰¹å¾´ãƒãƒƒãƒ—ã®1ã¤1ã¤ã®ç©ºé–“çš„ä½ç½®ã«å¯¾ã™ã‚‹è¾æ›¸ãƒ™ã‚¯ãƒˆãƒ«ã®è·é›¢
distance = (
    flatten.pow(2).sum(dim=1, keepdim=True)
    -2 * flatten @ embedding
    + embeddings.pow(2).sum(dim=0, keepdim=True)
)

# distanceã‹ã‚‰ã€1ã¤1ã¤ã®ç©ºé–“çš„ä½ç½®ã«å¯¾ã—ã¦ã€æœ€ã‚‚è¿‘ã„è¾æ›¸ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ã™ã‚‹
# embedding_idx: [N, K] --> [N, ]
embedding_idx = torch.argmin(distance, dim=1)

# è¾æ›¸ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒã‚’å…¥åŠ›ã•ã‚ŒãŸç‰¹å¾´ãƒãƒƒãƒ—ã«åˆã‚ã›ã‚‹
# embedding_idx: [N, ] --> [B, H, W, ]
embedding_idx = embedding_idx.view(*inputs.size()[:-1])

# 1ã¤1ã¤ã®ç©ºé–“çš„ä½ç½®ã«å¯¾å¿œã™ã‚‹è¾æ›¸Indexã«å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡ºã™ã‚‹
# è¾æ›¸Indexã¯[0, K-1]ã®å€¤ã‚’ã¨ã‚‹
# quantize: [B, H, W, ] x [K, D] --> [B, H, W, D]
quantize = F.embedding(embedding_idx, embedding.transpose(0, 1))

# é‡å­åŒ–ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’å…¥åŠ›ã•ã‚ŒãŸç‰¹å¾´ãƒãƒƒãƒ—ã«åˆã‚ã›ã‚‹
# quantize: [B, H, W, D] --> [B, H, W, D=C]
quantize = quantize.view(*inputs.size())
```

æå¤±å€¤ã¨å‹¾é…ä¼æ¬

```python
# é‡å­åŒ–å‰å¾Œã®ç‰¹å¾´ãƒãƒƒãƒ—ã®è¿‘ã¥ã‘ã‚‹
e_latent_loss = F.mse_loss(quantize.detach(), inputs         )
q_latent_loss = F.mse_loss(quantize,          inputs.detach())

# commitmentã®ä¿‚æ•°ã‚’è¨­ã‘ã‚‹
loss = q_latent_loss + commitment * e_latent_loss

# Decoderå´ã®é‡å­åŒ–ã•ã‚ŒãŸç‰¹å¾´ãƒãƒƒãƒ—ã«å¯¾ã™ã‚‹å‹¾é…ã‚’ãã®ã¾ã¾Encoderå´ã®å…¥åŠ›ã«æ¸¡ã™
# é€†ä¼æ¬ç”¨ã®è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã—ãªã„ã‚ˆã†ã«detach()ã‚’ä½¿ç”¨ã™ã‚‹
quantize = inputs + (quantize - inputs).detach()
```

EMA

```python
# 1ã¤1ã¤ã®ç©ºé–“çš„ä½ç½®ã®æ•°ã ã‘å­˜åœ¨ã™ã‚‹ã€è¾æ›¸ã¸ã®Indexã‚’OneHotãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹
# embedding_onehot: [N, ] --> [N, K]
embedding_onehot = F.onehot(embedding_idx, num_classes=num_emb)

# å„è¾æ›¸ãƒ™ã‚¯ãƒˆãƒ«ã¸ã®å‚ç…§å›æ•°ã®åˆè¨ˆã‚’è¨ˆç®—ã™ã‚‹
# reference_counts = [N, K] --> [K, ]
reference_counts = torch.sum(embedding_onehot, dim=0)

# æŒ‡æ•°ç§»å‹•å¹³å‡(EMA)ã‚’è¨ˆç®—ã™ã‚‹
# è¨ˆç®—å‰ã«EMAç”¨ã®å‚ç…§å›æ•°ã‚’åˆæœŸåŒ–ã™ã‚‹: cluster_size = torch.zeros(num_emb)
cluster_size = beta * cluster_size
               + (1 - beta) * reference_counts

# EMAç”¨ã®å‚ç…§å›æ•°ã®åˆè¨ˆã‚’è¨ˆç®—ã™ã‚‹
n = cluster_size.sum()

# ãƒ©ãƒ—ãƒ©ã‚¹å¹³æ»‘åŒ–ã‚’è¡Œã„ã€å‚ç…§å›æ•°ãŒ0ã®å ´åˆã§ã‚‚è¨ˆç®—ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãŠã
cluster_size = ((cluste_size + eps) / (n + cluster_size * eps)) * n

# OneHotãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ã£ã¦ã€å‚ç…§ã—ã¦ã„ã‚‹å„è¾æ›¸ãƒ™ã‚¯ãƒˆãƒ«ã®åˆè¨ˆã‚’è¨ˆç®—ã™ã‚‹
# dw: [D, N] x [N, K] --> [D, K]
dw = flatten.transpose(0, 1) @ embedding_onehot

# ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾ã—ã¦æŒ‡æ•°ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹
ema_embedding = beta * ema_embedding + (1 - beta) * dw

# å‚ç…§å›æ•°ã‚’æ­£è¦åŒ–ã™ã‚‹
# [D, K] / [1, K]
embedding = ema_embedding / cluster_size.unsqueeze(0)
```