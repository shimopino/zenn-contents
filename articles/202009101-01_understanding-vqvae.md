---
title: "VQVAEã®æ¦‚è¦"
emoji: "ğŸ‘‹"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["æ©Ÿæ¢°å­¦ç¿’"]
published: false
---


# VQVAE

## Feature Quanrizationéƒ¨åˆ†

```python
# D(=1ã¤ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ)=64, K(=ä¿å­˜ã™ã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°)=10
emb_dim, num_emb = 64, 10

# Feature Quantization Moduleã«ã¯ã€ç•³ã¿è¾¼ã¿å±¤ã®å‡ºåŠ›ã§ã‚ã‚‹ç‰¹å¾´ãƒãƒƒãƒ—ã‚’ä½¿ç”¨ã™ã‚‹
# inputs: [B, C=D, H, W] --> [B, H, W, C=D]
inputs = inputs.permute(0, 2, 3, 1).contiguous()

# ç‰¹å¾´ãƒãƒƒãƒ—ã‚’1æ¬¡å…ƒã«å¤‰æ›ã™ã‚‹
# flatten inputs feature map: [B, H, W, D] --> [N(=BxHxW), D]
flatten = inputs.view(-1, emb_dim)

# è¾æ›¸æ•°åˆ†æ ¼ç´ã™ã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
# embedding weight: [D, K]
embeddings = torch.randn(emb_dim, num_emb)
```